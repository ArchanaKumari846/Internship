{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82cd9f4c",
   "metadata": {},
   "source": [
    "# Image Scraping and Classification Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6d94c7",
   "metadata": {},
   "source": [
    "### Problem Statement: \n",
    "\n",
    "Images are one of the major sources of data in the field of data science and AI. This field is making appropriate use of information that can be gathered through images by examining its features and details. We are trying to give you an exposure of how an end to end project is developed in this field. \n",
    "\n",
    "The idea behind this project is to build a deep learning-based Image Classification model on images that will be scraped from e-commerce portal. This is done to make the model more and more robust. \n",
    "\n",
    "This task is divided into two phases: Data Collection and Mode Building. \n",
    "\n",
    "1. Data Collection Phase: \n",
    "\n",
    "In this section, you need to scrape images from e-commerce portal, Amazon.com. The clothing categories used for scraping will be:\n",
    "-\tSarees (women)\n",
    "-\tTrousers (men)\n",
    "-\tJeans (men)\n",
    "\n",
    "2. Model Building Phase: After the data collection and preparation is done, you need to build an image classification model that will classify between these 3 categories mentioned above. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805693fe",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a778003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from os import listdir\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4802ae",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d5c726",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data=r'Clothes/Train'\n",
    "test_data=r'Clothes/Test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f783850",
   "metadata": {},
   "source": [
    "## Displaying Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7f162e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's try to print some of the scrapped images from each category\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_jeans=r'Clothes/Train/Jeans_Images'\n",
    "train_saree=r'Clothes/Train/Sarees_Images'\n",
    "train_trouser=r'Clothes/Train/Trousers_Images'\n",
    "\n",
    "\n",
    "Cloth_train=[train_jeans, train_saree, train_trouser]\n",
    "for dirs in Cloth_train:\n",
    "    k=listdir(dirs)\n",
    "    for i in k[:3]:\n",
    "        img=mpimg.imread('{}/{}'.format(dirs,i))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e402b3d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file1 = listdir(r'Clothes/train')\n",
    "file2 = listdir(r'Clothes/test')\n",
    "file1, file2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7659bd0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count of images in each folder\n",
    "print(\"Count of Training Images\")\n",
    "print(\"No.of Images of Sarees in train dataset -> \",len(os.listdir(r'Clothes\\train\\Sarees_Images')))\n",
    "print(\"No.of Images of Jeans in train dataset -> \",len(os.listdir(r'Clothes\\train\\Jeans_Images')))\n",
    "print(\"No.of Images of Trousers in train dataset ->\",len(os.listdir(r'Clothes\\train\\Trousers_Images')))\n",
    "\"\\n\"\n",
    "\n",
    "print(\"Count of Test Images\")\n",
    "print(\"No.of Images of Sarees in test dataset-> \",len(os.listdir(r'Clothes\\test\\Sarees_Images')))\n",
    "print(\"No.of Images of Jeans in test dataset ->\",len(os.listdir(r'Clothes\\test\\Jeans_Images')))\n",
    "print(\"No.of Images of Trousers in test dataset-> \",len(os.listdir(r'Clothes\\test\\Trousers_Images')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62cc8a9",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0899c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importing all the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Activation, Conv2D, MaxPooling2D,BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import random\n",
    "import scipy\n",
    "import pylab as pl\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802ee510",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Defining dimensions of images and other parameters\n",
    "input_shape=(128,128,3)\n",
    "img_width=128\n",
    "img_height=128\n",
    "batch_size=12\n",
    "epoch=100\n",
    "train_samples=476\n",
    "test_samples=204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329ec494",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data Augmentation on Training Images\n",
    "\n",
    "Train_datagen=ImageDataGenerator(rescale=1./255,\n",
    "                                             zoom_range=0.2,\n",
    "                                             rotation_range=30,\n",
    "                                             horizontal_flip=True)\n",
    "Training_set=Train_datagen.flow_from_directory(train_data,\n",
    "                                                              target_size=(img_width,img_height),\n",
    "                                                              batch_size=batch_size, \n",
    "                                                              class_mode='categorical')\n",
    "\n",
    "# Test Data Generator\n",
    "Test_datagen=ImageDataGenerator(rescale=1./255)\n",
    "Test_set=Test_datagen.flow_from_directory(test_data,\n",
    "                                                  target_size=(img_width,img_height),\n",
    "                                                  batch_size=batch_size, \n",
    "                                                  class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0796da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "model=Sequential()\n",
    "\n",
    "# First convolution layer\n",
    "model.add(Conv2D(32,(3,3),input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Second convolution layer\n",
    "model.add(Conv2D(32,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Third convolution layer\n",
    "model.add(Conv2D(64,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Fourth convolution layer\n",
    "model.add(Conv2D(64,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef4f05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining Early stopping and Model check point\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "ES = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)\n",
    "MC = ModelCheckpoint('best.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70301b15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fitting the Training Data\n",
    "history = model.fit(\n",
    "    Training_set, \n",
    "    epochs=epoch,\n",
    "    validation_data=Test_set,\n",
    "    validation_steps=test_samples//batch_size,\n",
    "    steps_per_epoch=train_samples//batch_size,\n",
    "    callbacks=[ES,MC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c00de42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Saving the best model\n",
    "model.save('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff67346",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table = pd.DataFrame(model.history.history)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d95a496",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55b46eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Loading the saved model\n",
    "saved_model = load_model('best_model.h5')\n",
    "\n",
    "#creating instances where elements from test directory will be called\n",
    "test_jeans=r'Clothes\\test\\Jeans_Images'\n",
    "test_saree=r'Clothes\\test\\Sarees_Images'\n",
    "test_trouser=r'Clothes\\test\\Trousers_Images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e8225d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_dire=[test_jeans,test_saree,test_trouser]\n",
    "\n",
    "for test_dir in test_dire:\n",
    "    for i in listdir(test_dir):\n",
    "        print(\"Input Image is:\",i)\n",
    "        img= image.load_img('{}/{}'.format(test_dir,i))                         \n",
    "        test_image = image.load_img('{}/{}'.format(test_dir,i),target_size=(128, 128))\n",
    "        test_image = image.img_to_array(test_image)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        test_image = np.expand_dims(test_image, axis=0)\n",
    "        result = saved_model.predict(test_image)\n",
    "        print(\"Predicted Label is:\",np.argmax(result, axis=1),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d90ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
